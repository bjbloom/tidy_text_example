---
title: "Tidy Text Example"
author: "B.J. Bloom"
date: "October 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE)
```

## Simple Text Mining in R using `tidytext`

This is a quick example to show how easy it is to use `tidytext` for some basic Natural Language Processing in R. `tidytext` works well with other functions in the `tidyverse`, such as `dplyr`, `stringr`, `tidyr`, and `ggplot2`. I will demonstrate this using a movie reviews data set from the [`LDAvis`](https://ldavis.cpsievert.me/reviews/reviews.html) package. 

First, I'm going to load in the packages I need for this analysis, load in the data sets from the packages (including the `stop_words` data set included in `tidytext`), and transform the movie reviews data set from a character vector into a data frame that includes the name of the document as an identifier for it (important for joining back to the original data set after running the topic model).

```{r}
library(dplyr)
library(tidytext)
library(topicmodels)
library(ggplot2)
library(stringr)
library(tidyr)

data(reviews, package = "LDAvisData")
data("stop_words", package = "tidytext")

reviews_df <- data_frame(reviews)
reviews_df$names <- names(reviews)
```

### Tidy dataframe and get simple word frequencies/bigram frequencies

The biggest frustration that I have had with using `tm` as the primary package for text mining/NLP is that all of the pre-processing (remove stop words, move everything to lower case, remove punctuation, etc.) takes place after you convert it to a `tm` `Corpus`, which is an obscure, hard to manipulate object.

I do some simple cleaning within the data frame with `stringr` and then get a `tidytest` tidy tibble using the `unnest_tokens` function.

```{r reviews_df}
reviews_df <- reviews_df %>% 
  mutate(reviews = str_to_lower(reviews)) %>% 
  mutate(reviews = str_replace_all(reviews, '[^a-z]', ' '))

df <- reviews_df %>% 
  unnest_tokens(word, reviews) %>% 
  anti_join(stop_words, by = "word") %>% 
  count(names, word) %>% 
  ungroup()
```

The resulting data frame, `df`, is a `tidytext` tidied tibble, with one word per row per document. This is a very long, rather than wide, data frame, but one that is now really easy to manipulate to get some simple aggregate information (like word frequencies) and which can be used as multiple inputs for different NLP tasks.

One of my big frustrations with using `tm` is that, while you can do some pre-processing on the Corpus, the simple task of reducing your set of vocabulary to only words about a certain count is actually quite difficult (usually involves `strsplit`, `unlist`, and one or more `apply` functions), simply because `Corpus` is a hard object type to work with. A tidy data frame is very simple to work with and pipes well for `dplyr`, `tidyr`, and `ggplot2`.

Here, I create a simple work frequency data frame both for visualization purposes and later to filter out uncommon words to create a document-term matrix

```{r df}
word_freq_df <- df %>% 
  group_by(word) %>% 
  summarize(count = n()) %>% 
  filter(count > 20)

word_freq_df %>% 
  mutate(word = reorder(word, count)) %>% 
  top_n(10, count) %>%
  ggplot(aes(word, count)) +
  geom_bar(stat = "identity", fill = "red") +
  ggtitle("Word Frequency in Movie Reviews") +
  coord_flip()
```

Going back to the initial review data frame, I can instead tokenize on bi-grams and use `tidyr` to `separate`, remove stop words from each column of words in the bigram, then `unite` back and get a simple visualization of bi-grams in the corpus

```{r }
bigram_df <- reviews_df %>% 
  unnest_tokens(bigram, reviews, token = "ngrams", n = 2) %>% 
  count(bigram, sort = TRUE)

bigram_word_freq <- bigram_df %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  unite(bigrams, word1, word2, sep = " ")

bigram_word_freq %>% 
  mutate(bigrams = reorder(bigrams, n)) %>% 
  top_n(10, n) %>%
  ggplot(aes(bigrams, n)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  ggtitle("Bi-gram Frequency in Movie Reviews") +
  coord_flip()
```

### Topic modeling and outputting useful results of the topic model

Creating a document-term matrix (or term-frequency, inverse document frequency (tf-idf) matrix) is certainly easy to do from a `tm` `Corpus` object for use as an input into the `topicmodels` package's version of latent dirichlet allocation (LDA), a common implementation of a topic model (note: this doesn't work as cleanly for the `LDA` package's implementation of an LDA model, and one shortcoming of `tidytext` is that there is no simple solution for creating the necessary inputs for the `LDA` version of the model. I may discuss this in another post, but `textmineR` is very useful for that). But it's even easier using `tidytext`. And because we retain both the original data frame and the tidy'd version of the tokenized data frame, it's to relate the results back to the original data, using some simple `dplyr` functions.

```{r }
#Use filtered word frequency data frame to get dtm
df <- df %>% 
  inner_join(word_freq_df, by = "word") %>% 
  select(names, word, n)

#Cast as document-term matrix
df_dtm <- df %>% 
  cast_dtm(names, word, n)

#Topic model
review_lda <- LDA(df_dtm, k = 20, method = "Gibbs", control = list(seed = 1234))
```

When building a topic model, one of the hardest things is dealing with the obscure S3 objects that the output generates. It's possible to pull things out of the list object, it's just not that easy. `tidytext` implements some `broom`-like operations for doing just this, which can easily be joined back to the original data frame. We can do this both for `beta` (probability of a word given a topic) and for `gamma` (probability of a topic given a document) which helps us understand the topic model we just created.

I create two data frames: one containing `beta` and one containing `gamma`:

```{r review_lda}
#Broom-like method to return probability of word being in topic
review_lda_word <- tidy(review_lda)

#Broom-like method to return probability of topic given a document
review_lda_topic <- tidy(review_lda, matrix = "gamma")
```

Topic models are hard to explain to your end users in general. It's particularly difficult to explain what they really mean. "What is topic X?" Well, there is a known distribution of words that are likely to be included in topic X. "But what do I call it?" One way to answer this question is to provide the top n most probably terms in each topic.

A second common question is "Now that I know what each topic is, can I see all documents in topic X?" Of course, a topic model assumes that there is a distribution of topics across documents, so each document is composed of multiple topics. But in terms of real world application, often your end user wants to prioritize a work stream by assigning someone to further investigate a set of documents. Narrowing down which documents to review is a very helpful application of a topic model. 

```{r review_lda_word}
#Look at the most common words per topic, for just one topic
review_lda_word %>% 
  filter(topic == 1) %>% 
  top_n(10, beta) %>% 
  #arrange(desc(beta)) %>% 
  mutate(term = reorder(term, -beta)) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", fill = "blue", show.legend = F) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1, size = 15)) +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r review_lda_topic}
#Output topic probabilites for each document, limit to 5 and threshold for gamma

classifications <- review_lda_topic %>% 
  group_by(document) %>% 
  filter(gamma > 0.3) %>% #set threshold (otherwise lots of topics/document)
  top_n(5, gamma) %>%      #take up to 5 topics per document, as long as gamma > 0.05
  arrange(desc(gamma)) %>% 
  mutate(gamma_rank = row_number()) %>% #rank order gamma per document
  ungroup()

#Re-join to original data set to return to original text of review
#Can use this to filter by topic for review

review_topics <- classifications %>% 
  left_join(reviews_df, by = c("document" = "names")) %>% 
  select(document, topic, gamma, reviews, gamma_rank)
```

I have primarily used this data analysis method to then create an R Shiny app, in which case I use a user input to filter for the topic number in a tab for the word frequency per topic (`filter(topic == input$foo` in the code chunk instead). While long documents aren't ideal to be placed into a data frame, I've built topic models on smaller pieces of text (web form data, in more than one case), so it was OK (but not great) to display at a `DT` data table and then provide an option to download a filtered CSV of the resulting topic. I create the `gamma_rank` field in the above topic-specific data frame to create a filter if the user wants to review fewer documents (creating a conditional check box where `gamma_rank` == 1 would only assign one topic to each document, which is the topic with the highest gamma value for that document).

I hope this demonstrates some of the functionality of `tidytext`. I'm hardly doing anything innovative here. I just like the `tidyverse` in general, and find this to be a useful framework for doing NLP-related analysis.